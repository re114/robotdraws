{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "robot draws introduction.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wszdAKEz0RN6"
      },
      "source": [
        "# Summary\n",
        "Line or vector drawings are a way to render images in an efficient abstracted form, and in doing so highlight specific features or aspects of an image.\n",
        "\n",
        "Machine Learning does the same with data by determining the most efficient representation - the latent space - which allows the proximity of data features to be discovered, and analysed.\n",
        "\n",
        "I am investigating both to identify effective methods of generating line drawings using machine learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iub444CZ1h_1"
      },
      "source": [
        "#Motivation\n",
        "In 2019 we celebrated the 150th anniversary of the birth of artist John Ruskin. Ruskin outlined a seminal, systematic approach to teaching drawing in the foundational text ['Elements of Drawing'](https://) (Ruskin, 1856) which focused on pen skills and observation. \n",
        "When we draw we make marks that render images in an efficient abstracted form, and in doing so highlight specific features or aspects of an image.\n",
        "\n",
        "Both penskills and observation can be emulated by machines, but the harder part of the problem is observation and related perception and inference; knowing what is being drawn and why it looks the way it does.\n",
        "\n",
        "Machine learning is an area of Artificial Intelligence that is “trained rather than explicitly programmed” (Francois Chollet, 2018). \n",
        "Given a body of relevant data it finds statistical structures and returns probabilities.\n",
        "In essence Machine Learning aims to determine the most efficient representation of compressed data - the latent space - which allows the proximity of data features to be discovered, and analysed.\n",
        "\n",
        "Deep Learning is a subset of Machine learning that utilises multiple layers of learning to determine more precise representations of the data from each subsequent layer. (The depth has nothing to do with the quality of the understanding, just the number of layers through which learning takes place).\n",
        "\n",
        "The same can be said of a sketch drawing. It pares an image to the most essential elements for a specific purpose. \n",
        "A cat can be represented by a few simple lines.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7nWHgGm20da"
      },
      "source": [
        "Both can be seen as processes in a visual information communication system, that take a set of data, refine and process it to produce an output.(SHANNON, C. E., & WEAVER, 1963)\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/re114/robotdraws/main/img/shannon-weaver-information-flow.png \"information flow\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RxakU3oCAMg"
      },
      "source": [
        "On this site I will review the intrinsic traits of images and how they are represented, perceived, analysed and synthesized in nature and compare this to the state of the art for generating sparse efficient vector drawings from bitmap data using machine learning, and suggest and test improvements and efficiencies to the generation process.\n",
        "\n",
        "Whilst the human capacity for vision and drawing are the standard against which we aim to be measured it should be remembered that evolution generates an effective outcome for the selection pressures rather than a design for a specific function.\n",
        "\n",
        ">\"all animals are under stringent selection pressure to be as stupid as they can get away with\" (Richerson & Boyd, 2005)\n",
        "\n",
        "\n",
        "In the human model, light reflects from a surface is focussed in the retina, interpreted by the brain, and initiates movement of the drawing tool (Marr, 1982).\n",
        "\n",
        ">\"the visual system generates a sequence of increasingly symbolic representations of a scene, progressing from a ‘primal sketch’ of the retinal image, through a ‘2 1⁄2D sketch’ to simplified three-dimensional models of objects.\"  \n",
        "(Marr & Hildreth, 1980)\n",
        "  \n",
        ">\"At the time of [Marr's] publication, Binford, Horn, Minsky, Papert, Rumelhart and others had been advocating computational modelling as a key to understanding the brain's operation but Marr brought a number of different approaches together, made testable predictions, provided a framework for tackling challenging neuroscientific questions and inspired a generation of young scientists to study the brain and visual processing.\"  \n",
        "(Glennerster, 2007)\n",
        "\n",
        "I will follow Marr's model - and review perception, interpretation, synthesis, as highlighted by Glennerster:\n",
        "\n",
        ">\"Of his ‘theoretical maxims’, the best known is Marr's argument that problems in neuroscience must be tackled at a number of different levels: \n",
        "computational theory,  \n",
        "algorithm   \n",
        "and implementation.   \n",
        "Computational theory means making explicit the input and output of a process and the constraints that would allow the problem to be solved.  \n",
        "This analysis must come first, he claimed.  \n",
        "The algorithmic level describes in more detail how to get from input to output but it should be independent of the implementation, the third level.\"  \n",
        "(Glennerster, 2007)\n",
        "\n",
        "Much has changed in the 40 years since Marr's seminal work, but the core problem for both the neuroscientist and the computer scientist remains the same:\n",
        "\n",
        ">\"...the basic problem that the brain has to solve is the induction of extrinsic properties from intrinsic properties; \n",
        "roughly, inferring the distal stimulus from its neural coding. \n",
        "Accordingly, this decomposes into three sub-problems:  \n",
        "(i) the discovery problem—the picking out of collections of frequent, closely similar subevents;  \n",
        "(ii) the representation problem—how best to represent tokens of the same type of subevent in a single classificatory unit;  \n",
        "and  \n",
        "(iii) the diagnosis problem—how best to decide whether a new subevent belongs to particular class.\"  \n",
        "(Peebles & Cooper, 2015)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bVe3RuEFhs3"
      },
      "source": [
        "##Intrinsic traits and domain unique challenges\n",
        "In this section I introduce the processes required to gather and form a representation of an image.  \n",
        "Shannon and Weaver described the basic model for the transmission of information in their key work the mathematical theory of communication. (SHANNON, C. E., & WEAVER, 1963) but are very clear that the meaning of the information is part of a wider system.  \n",
        "Our information source is reflected light and we aim to gather useful information from it.  \n",
        "We should be aware that visible light is a small part of the electromagnetic spectrum, perception which in humans ranges from violet at 360 to a deep red at 760 nanometres (nm), (Sliney, 2016) this is not the only range we could process, it is the range we have evolved to utilise. We can compare this to the honey bee's compound eyes which cover three ranges - Ultraviolet peaking at 344 nm, Blue peaking at 436, and green at 544nm (Hempel de Ibarra, Vorobyev, & Menzel, 2014). Whilst we will concentrate on human visible light it should be remembered that machines are not limited to our visual range, and can easily see in ultraviolet or infrared, in fact any part of the electromagnetic spectrum.  \n",
        "Similarly, the processing of the data depends on the requirements of the organism. Evolution delivers a system honed by need - the fly doesn't need to process it's images, and can flourish by reacting to visual stimuli (Reichardt, Poggio, & Hausen, 1983). It has five \"independent, rigidly inflexible and very fast responding systems (@21ms) (Marr, 1982) to specify whether the visual field is looming sufficiently fast to initiate landing, or whether a speck can be intercepted.  \n",
        "The fly has no explicit visual representation of the physical world, just optical triggers.  \n",
        "So we look at Light between 360 and 760 nm, and also consider the complication of a layer of consciousness to process the data, which ignores improbable stimuli at a conscious level (Blakemore, Adler, Pointon, & Ramachandran, 2010) also (Purves, Lotto, Williams, Nundy, & Yang, 2001).  \n",
        "Data gathering isn't as straightforward as might be expected, and the eye constantly flits around in a saccadic twitch [Yarbus, A.L. 1967 eye movements during the perception of complex objects (Yarbus & Yarbus, 1967).  \n",
        ">\"In the process of being transmitted, it is unfortunately characteristic that certain things are added to the signal which were not intended by the information source. These unwanted additions may be ... distortions in shape or shading of picture (television), or errors in transmission (telegraphy or facsimile), etc. All of these changes in the transmitted signal are called noise.\"  \n",
        "(SHANNON, C. E., & WEAVER, 1963).\n",
        "\n",
        "It is possible we need to process the data to deal with the noise inherent in transmission, although some argue the slow and costly nature of conscious processing doesn't seem to have any immediate evolutionary benefit over non conscious processing, (Matsumoto, 2004) but rather makes use of the limbic reward system as a pathway for appreciating aesthetics.  \n",
        "According to Altenmuller (Altenmuller, 2003), we appear to be accidentally hard wired to appreciate artistic effort but this wiring may only be a relatively recent phenomenon. The earliest extant art, seen in the Lascaux caves date to around 15000 BC and are line drawings.  \n",
        "They could be line drawings either due to the sparsity of the drawing material, or the immediacy of the act, or because of the efficiency of representation. (Bastian & Alabouvette, 2009)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1DcNTPWGZR_"
      },
      "source": [
        "###Physical components\n",
        "The eye gathers data in the form of light (between 360 and 760 nm ) which is focussed with a lens on the retina, comprised of rods and cones and 60 types of layered cells which are triggered by colour and light. The data is passed through optic nerve to the brain. Shannon and Weaver give us an approach to calculate the channel capacity, coding and noise, and determine how much information the system needs to parse in terms of bits and bandwidth which appears to be around 8.75 megabits per second (Reilly, 2006), similar to the throughput of a 4 Megapixel video camera using H264. Schmidt outlines the need to sift the incoming data due to the channel limitations (Schmidt & Fleming, 2016), and this is confirmed by work on retinal prostheses (Tong, Meffin, Garrett, & Ibbotson, 2020)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQyyIuuAGo06"
      },
      "source": [
        "###representational framework for vision\n",
        "Marr suggests :\"The aim [of vision] is to develop a useful canonical description of the shapes and surfaces then form the image\" (Marr, Palm, & Poggio, 1978)  \n",
        "He proposes four factors responsible for intensity values of an image:  \n",
        "1. Geometry  \n",
        "2. Reflectance of visible surfaces  \n",
        "3. Illumination of the Scene  \n",
        "and  \n",
        "4. Viewpoint  \n",
        "A primal sketch is formed from detecting changes in the intensity of the first three factors, and the primal sketch is then processed based on viewpoint to determine a 2 1/2D sketch.  \n",
        "NOTE The artist has direct control over the fourth factor, and actively changes it to confirm and reinforce perceptions by stereopsis during sketching. This may provide a method for improving perception in machine vision systems.\n",
        "This retinocentric viewpoint/frame of reference describes spatial relations in 2D in relation to the observer’s retina.  \n",
        "Marr again: “We see smoothed out surfaces as demonstrated by (Tyler 1973) who determined we cannot resolve \"corrugations higher than 4 cycles per degree in a stereogram.\"(Marr, 1982)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFb7jtj-G9Bf"
      },
      "source": [
        "###Electronic representation of images\n",
        "Kenneth Craik (BARNES, 1944) highlighted the commonality machines share with the brain in some functions, where \"the fundamental nature of neural machinery has the power to parallel or model neural events\".\n",
        "The data input aspect of electronic images allows a wider range of systems to be described.\n",
        "An analogue of the human vision system would start with a photoreceptor, CCD or CMOS that breaks an image into a bitmap."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhAm9a6fHHaR"
      },
      "source": [
        "###Traits of raster/bitmap images.\n",
        "Raster or bitmap images are two dimensional grids of pixels (picture elements) which hold values for colour and brightness. These values are usually described as Red Green and Blue (RGB) in additive screen-based images, and Cyan Yellow Magenta and Black (CYMK) for subtractive print methods. Other schema such as the grayscale used in the Modified National Institute of Standards and Technology database (MNIST) (LeCunn, Cortes, & Burges, n.d.) data set of handwritten digits used for training and testing image processing systems and machine learning model, represent the pixel brightness by a single number between 0 and 255.\n",
        "![MNIST digits](https://raw.githubusercontent.com/re114/robotdraws/main/img/MNIST-digits-and-labels.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlaGpKz3HbCw"
      },
      "source": [
        "MINIST uses a 28x28 grid to represent its digits,\n",
        "So, for example a representation of the figure 5, the first digit in the MNIST array would be:  \n",
        "![MNIST digit](https://raw.githubusercontent.com/re114/robotdraws/main/img/MNIST-28-by-28-greyscale.png)\n",
        "The density of the pixels in digital images tends to map roughly to the acuity of the human eye, and generally ranges from @ 72dpi for screen-based images to @ 600dpi for print. The detail resolved is also dependent on the distance between the display and the eye of the person (or lens of the device) viewing the display."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y09PJTHoJcKm"
      },
      "source": [
        "###Traits of vector images\n",
        "Bitmaps provide a useful data storage model but not the only option. Mathematical vectors are objects that have a magnitude and a direction (Fox & Bolton, 2002). These can be used to efficiently record the pen strokes required to make a sketch. As such they provide a more direct output format for sketches than bitmaps.  \n",
        "Practical implementations of these vectors are usually handled by implementing an interoperable specification, such as Scalable Vector Graphics (SVG) and GCODE.  \n",
        "\n",
        "####SVG\n",
        "![Example of SVG drawing.](https://raw.githubusercontent.com/re114/robotdraws/main/img/example-svg.png)  \n",
        "Scalable Vector Graphics (SVG) are the de-facto specification for describing 2D vector graphics.  \n",
        "SVG is a human readable text based Extensible Mark-up Language (XML) open web standard, maintained by the World Wide Web Consortium (W3C). A set of elements are described by their attributes, and transformations. (“SVG: Scalable Vector Graphics | MDN,” n.d.)\n",
        "\n",
        "\n",
        "```\n",
        "<?xml version=\"1.0\" standalone=\"no\"?>\n",
        "<svg width=\"200\" height=\"250\" version=\"1.1\" xmlns=\"http://www.w3.org/2000/svg\">\n",
        "  <rect x=\"10\" y=\"10\" width=\"30\" height=\"30\" stroke=\"black\" fill=\"transparent\" 2Dstroke- width=\"5\"/>\n",
        "  <rect x=\"60\" y=\"10\" rx=\"10\" ry=\"10\" width=\"30\" height=\"30\" stroke=\"black\" fill=\"transparent\" stroke-width=\"5\"/>\n",
        "  <circle cx=\"25\" cy=\"75\" r=\"20\" stroke=\"red\" fill=\"transparent\" stroke-width=\"5\"/>\n",
        "  <ellipse cx=\"75\" cy=\"75\" rx=\"20\" ry=\"5\" stroke=\"red\" fill=\"transparent\" stroke- width=\"5\"/>\n",
        "  <line x1=\"10\" x2=\"50\" y1=\"110\" y2=\"150\" stroke=\"orange\" stroke-width=\"5\"/>\n",
        "  <polyline points=\"60 110 65 120 70 115 75 130 80 125 85 140 90 135 95 150 100 145\" stroke=\"orange\" fill=\"transparent\" stroke-width=\"5\"/>\n",
        "  <polygon points=\"50 160 55 180 70 180 60 190 65 205 50 195 35 205 40 190 30 180 45 180\" stroke=\"green\" fill=\"transparent\" stroke-width=\"5\"/>\n",
        "  <path d=\"M20,230 Q40,205 50,230 T90,230\" fill=\"none\" stroke=\"blue\" stroke-width=\"5\"/> \n",
        "</svg>\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUWDl5hcLCvq"
      },
      "source": [
        "####GCODE  \n",
        "G-code encompasses a range of languages and dialects that developed from work at MIT (Shanker Dixit, Hazarika, & Davim, 2017) in the 1950's to enable precision manufacture with Computer Numerical Control (CNC) and Computer Aided Manufacture (CAM) machines. At its core are a set of commands that indicate how and where to move a machine head, and at what speed. For instance, a 3D printer or a mill might be instructed to draw a circle:  \n",
        "\n",
        "\n",
        "```\n",
        "(---------- START OF CODE ----------)\n",
        "G21 (mm)\n",
        "G0Z0.1000\n",
        "G0X0.0000Y0.0000Z0.1000\n",
        "G0 X0.5625Y1.0000\n",
        "G1 Z-0.0625F5.00\n",
        "F5.00\n",
        "G2 X0.5625Y1.0000 i0.4375j0 z-0.0625 G0Z0.1000 (Retract)\n",
        "G0X0Y0 (Origin)\n",
        "M30 (End of Program)\n",
        "(---------- END OF CODE ----------)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yg5P9Mw4LT5g"
      },
      "source": [
        "As a point of interest in 2008 I worked on an interoperability specification for Interactive Whiteboards. At the time the two major players Promethean and SMART used proprietary standards for exports, which limited the effective use of resources created by teaching staff in sites with both technologies. The result of the project was that the most effective way of recording IWB sketches was SVG, with some extensions.  \n",
        "The project was handed over to IMS on the dissolution of Becta. \"[https://www.imsglobal.org/IWBCFF/index.html](https://www.imsglobal.org/IWBCFF/index.html)\"  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_7dymrDLk86"
      },
      "source": [
        "##Sketching  \n",
        "Sketching tends to be used to describe preparatory drawings, and drawings themselves are often viewed as a preparation for some other finished work. In this study we use the terms synonymously to describe line art:\n",
        "“the formation of a line by drawing some tracing instrument from point to point of a surface; representation by lines; delineation as distinguished from painting...the arrangement of lines which determine form.”(Shorter Oxford English Dictionary, 2007)  \n",
        "\n",
        "Ruskin was determined that anyone could learn to draw and established a Drawing School in Oxford in 1871. His teaching method required students to systematically study outline, shading and colour through a course of lessons laid out in his seminal work \"Elements of Drawing\".  \n",
        "\n",
        "The \"first large scale exploration of sketches: ‘How Do Humans Sketch Objects?' (Eitz, Berlin, Hays, & Alexa, 2012), used Amazon Mechanical Turk to generate a substantial dataset of 20000 images for 250 categories (types of thing, apple, cat etc) which equates to around 53 sketches for each.   \n",
        "From the large dataset the team tested human capacity to identify the images, with an accuracy of 73%.  \n",
        "\n",
        "So, whilst according to Ruskin anyone can learn to draw, perhaps not everyone does. This could be a mechanical issue or perhaps their internal shorthand representation for the object differs from that of the viewers?  \n",
        "\n",
        "Eitz et al attempted to model sketch recognition and used a \"bag-of-features” sketch representation and multi-class support vector machines trained on the sketch dataset\" to classify sketches. The resulting model scored an average of 56% accuracy and using the model the team demonstrated a basic sketch recognition system.  \n",
        "Whilst the average accuracy was 56% the variation was quite extreme, apples scoring high with an accuracy of 0.96296 (96%), but cats scoring a poor 0.2222 (22%). It appears that the issue is a mixture of poor draughtsmanship and bad internal models. The need for improved accuracy was clear.  \n",
        "![Eitz-et-al-images](https://raw.githubusercontent.com/re114/robotdraws/main/img/Eitz-images.png)  \n",
        ">“Top row: six most difficult classes for human recognition. E.g., only 2.5% of all seagull sketches are correctly identified as such by humans. Instead, humans often mistake sketches belonging to the classes shown in the rows below as seagulls. Out of all sketches confused with seagull, 47% belong to flying bird, 24% to standing bird and 14% to pigeon. The remaining 15% (not shown in this figure) are distributed over various other classes.”  \n",
        "(Eitz et al., 2012)\n",
        "\n",
        "\n",
        "Whilst support vector machines have largely been surpassed by convolutional neural networks, data creation processes seem to rely on low cost piece work. Li et al in their 2019 work Photo-Sketching: Inferring Contour Drawings from Images, also used Amazon Mechanical Turk to generate a corpus of 5000 labelled tracings of images to map features this time from bitmap images. Their use of Convolutional Neural Networks (CNNs- see sections 2.4.3 and 2.5) improved the result, which whilst better are still not artistic. (M. Li, Lin, Měch, Yumer, & Ramanan, 2019).\n",
        "\n",
        "\n",
        "![Examples of edge and boundary detection methods compared to Li et al contour drawing.](https://raw.githubusercontent.com/re114/robotdraws/main/img/Li-et-al-images.png)  \n",
        "\n",
        "The Google Brain research project ‘Magenta’ investigated Artificial Intelligence in creativity, and as part of this used the popular “Quick Draw” online game to train a far bigger (deeper) Recurrent Neural Network (RNN)(Ha & Eck, 2017). Despite having access to a training set of 50 million sketches in the form of timestamped vectors, the outputs are generally poor. For example, if you run the variational autoencoder with a cat model, and a simplistic cat sketch as the input the resulting outputs are recognisable as cats, albeit poorly drawn cats (Ha, Jongejan, & Johnson, 2017)  \n",
        "\n",
        "![Examples of cats created using the sketch rnn autoencoder.](https://raw.githubusercontent.com/re114/robotdraws/main/img/sketch-rnn-vae.png)  \n",
        "\n",
        "From the literature and the experimental results, it seemed apparent that after the undoubtedly impressive initial successes provided by deep learning, some problems did not lend themselves to increasingly greater brute force. Increasingly larger datasets and faster networks did not produce noticeably better outputs but diminishing marginal returns. \n",
        "\n",
        ">“For most problems where deep learning has enabled transformationally better solutions (vision, speech), we've entered diminishing returns territory in 2016-2017.”  \n",
        "(François Chollet, 2019)  \n",
        "\n",
        "That has changed somewhat with the advent of large scale transformers, but at a cost, GPT3 is rumoured to have cost somewhere between \$4 and \$12 million to train, and used an estimated 335 processor years. [source]\n",
        "\n",
        "\n"
      ]
    }
  ]
}
